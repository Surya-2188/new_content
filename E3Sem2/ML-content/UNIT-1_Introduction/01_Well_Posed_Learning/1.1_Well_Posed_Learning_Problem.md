
# Chapter 1: Introduction to Machine Learning & The T–P–E Framework


## 1. Introduction: What Do We Mean by Learning?


We use the word learning all the time — *“I learned to swim”*, *“I learned to sing”*, *“I learned coding”*.


Intuitively, we already know:
* There is a **task** we want to perform.
* We get **practice or experience** doing it.
* And over time, our **performance improves**.


**This improvement is the key.**


> **✔️ Example: Eating**
> “Eating” is an activity, but it becomes *learning* only if:
> 1.  We practice something (e.g., eating faster, eating carefully, eating with chopsticks).
> 2.  And improve with respect to a performance measure (time taken, cleanliness, etiquette).


**So not every human activity is learning.** Only those activities where performance improves with experience. This is true for most areas of human evolution:
* We learned to hunt more efficiently.
* We learned to build better tools.
* We learned agriculture, architecture, and medicine.
* Civilization itself grew because humans improved tasks with respect to performance over generations.


**Thus:**
$$Learning = Task + Performance \ Measure + Improvement \ through \ Experience$$


---


## 2. Why Bring Learning Into Computers?


Computers repeat tasks every day:
* Running a Google Sheet financial script.
* Recommending videos.
* Predicting weather.
* Detecting spam mail.


They clearly have **experience** (data).
They clearly have a **task** (predict, classify, recommend).
**But do they have a performance measure?**


That became the fundamental question.
* **In humans:** If someone gives better investment insights after 10 years $\rightarrow$ we say they “learned”.
* **In computers:** If a program gives better insights the more it runs $\rightarrow$ it has also “learned”.


This idea opened the door for machine learning.


---


## 3. Tom Mitchell’s Formal Definition


Tom Mitchell formalized this intuitive idea into something precise:


> **“A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”**


This is the **T–P–E framework**. To call something “learning”, we must define:






| Component | Meaning | Example (Human) | Example (Machine) |
| :--- | :--- | :--- | :--- |
| **T – Task** | What activity are we performing? | Singing a song | Classifying images |
| **P – Performance** | How do we judge improvement? | Pitch accuracy | Accuracy / F1 score |
| **E – Experience** | What data/practice improves performance? | Daily singing practice | Labeled training data |


**Without defining these 3 things, we cannot mathematically say learning occurred.**


---


## 4. Why Do We Need “Well-Posed” Learning Problems?


Humans know learning intuitively. **But computers need mathematical clarity.**


A **well-posed ML problem** must clearly specify:
1. **Task (T)** $\rightarrow$ the operation the program must perform.
2. **Performance measure (P)** $\rightarrow$ how success is evaluated.
3. **Experience (E)** $\rightarrow$ what data or interactions improve the model.


If any one of these is vague, the learning problem becomes meaningless.


> **Example:**
> * *“I want an AI to understand traffic.”*
>     $\rightarrow$ **Not well-posed** (what does “understand” mean?)
>
> * *“I want to Classify vehicles (T), measured by Accuracy (P), using labeled images (E).”*
>     $\rightarrow$ **Well-posed ML problem.**


---


## 5. Human Learning Examples Mapped to T–P–E


### ✔️ Example 1: Singing
* **Task (T):** Sing a song.
* **Performance (P):** Pitch accuracy, rhythm stability.
* **Experience (E):** Daily voice practice, feedback from teacher.
* *Result:* If pitch deviations reduce over time $\rightarrow$ **learning occurred**.


### ✔️ Example 2: Dancing
* **Task (T):** Perform a dance routine.
* **Performance (P):** Smoothness, timing with music.
* **Experience (E):** Repeated practice with corrections.
* *Result:* If unit movements become more synchronized $\rightarrow$ **learning occurred**.


### ✔️ Example 3: Cooking
* **Task (T):** Prepare a dish.
* **Performance (P):** Taste, cooking time, consistency.
* **Experience (E):** Trying the dish repeatedly, adjusting spices.
* *Result:* If your dish keeps improving $\rightarrow$ **learning occurred**.


---


## 6. The Broader Landscape of Learning


Why do we bother defining T, P, and E so carefully? Because this framework allows us to solve massive, real-world problems.


**Successful Applications:**
* **Speech recognition:** Siri/Alexa understanding your voice.
* **Medical prediction:** Diagnosing diseases from X-rays.
* **Fraud detection:** Catching fake credit card transactions instantly.
* **Autonomous driving:** Cars navigating traffic safely.
* **Astronomy:** Classifying galaxies from telescope data.


**Machine Learning is Multidisciplinary:**
It is not just computer science. It draws deeply from:
* **Artificial Intelligence** (Reasoning)
* **Statistics** (Probability)
* **Neurobiology** (How the brain works)
* **Information Theory** (Data compression)
* **Philosophy & Psychology** (How we understand knowledge)


**The Goal of the Field:**
To understand and design algorithms that can learn effectively from experience.


---


## 7. Conclusion: The Road Ahead


We have established that humans learn naturally, but computers require a formal definition. Tom Mitchell provided this operational definition using **T (Task), P (Performance), and E (Experience)**.


### What comes next?
Now that we have the English definition, we must translate it into the language of computers: **Mathematics.**


In the next chapter, our objective is to take this simple T-P-E concept and convert it into a **mathematical formula** (involving function approximation and hypothesis spaces).


### Does that sound scary?
It can be. The math involves theories on:
* **Data Size:** How much experience ($E$) do we need?
* **Hypothesis Complexity:** How difficult is the task ($T$)?
* **Expected Error:** Can we guarantee the performance ($P$)?





**Remember:**
If you cannot break your problem down into **T, P, and E**, you cannot solve it with Machine Learning.


