 # The Final Design: A Team of Learning Agents


Let us now imagine that we are designing not just a single chess program, but a **team of cooperating agents** whose only job is to train one main player—the Grandmaster Agent.


This Grandmaster Agent does only one thing: **It plays the game using the current evaluation function ($\hat{V}$
).**


It looks at a board position, uses **$\hat{V}$** to score it, and picks the move that leads to the best next position. That is all. It does not know how to improve itself—other agents will take care of that.


The team consists of four distinct components:
1. **Performance Agent:** Plays the game.
2. **Critic Agent:** Evaluates the game history.
3. **Generalizer Agent (LMS):** Updates the learning weights.
4. **Experiment Generator:** Creates new learning scenarios.






Let us walk through how they cooperate.


---


## 1. The Performance Agent (The Intern Who Performs the Operation)


Think of the Performance Agent as a **PG medical student performing a surgery under supervision**.


The student:
* Performs the entire procedure.
* Does not know precisely which steps were best or worst at the moment.
* Simply follows the knowledge they currently have.


**In Chess Terms:**
The Performance Agent plays the full game against itself. It generates a sequence of board positions by using the current evaluation function (**$\hat{V}$**) to choose moves.
It produces a **Game History**:
> **History = (Board 1, Board 2, Board 3, ... Board T)**


This is like the student performing all steps of the surgery, recording what happened at each step.


---


## 2. The Critic Agent (The Senior Doctor Who Reviews Every Step)


After the surgery (game) is complete, the **Professor (Critic)** comes in to evaluate.


Does the Critic know the true quality of every step?
**No**—and this is the key. Just like in chess, we only know the final outcome (Win/Loss). The intermediate steps are unclear.


So the Critic must **estimate** how good each move was. It does this by **bootstrapping** using the current evaluation function:
> **$V_{\text{train}}$ = $\hat{V}$
(Successor(b))**


**In Human Terms:**
The Critic says:

* "The move at step 20 led to a horrible position later -> **Give it a low score.**"
* "The move at step 5 led to a strong position -> **Give it a high score.**"
* "The end position is a win -> **Earlier moves leading toward it get positive credit.**"


This Critic transforms the raw game history into a set of **Labeled Training Examples**.


---


## 3. The Generalizer (LMS) — Updating the Weights


Now that the Critic has labeled every intermediate step, the **Generalizer** performs the actual learning.


The Generalizer:
* Takes the evaluation errors found by the Critic.
* Adjusts the feature weights.
* Improves the scoring function (**$\hat{V}$**).


This corresponds to the LMS update rule we defined earlier:
> **w_new = w_old + Learning_Rate * (Error) * Feature_Value**


**In Human Terms:**
* "Give more weight to this symptom next time."
* "This positional idea in chess is more important—increase that coefficient."
* "This feature misled you—reduce its influence."


At the end of this step, we have a **new hypothesis**: A newly updated evaluation function with better weights. The Grandmaster Agent has effectively "learned."


---


## 4. The Experiment Generator — Sending the Next Patient


Once the weights are updated, the system needs more practice.
* **In a hospital:** The Head of Department (HOD) assigns a new patient case.
* **In chess:** The Experiment Generator starts a new game.


In simple designs, it always starts from the standard initial board. In more advanced systems, it may generate:
* Tricky positions.
* Unusual openings.
* Dangerous middle-game situations.


This ensures the agent (student) isn't just memorizing one type of game but is being challenged with diverse scenarios.


---


## Summary: The Full Learning Cycle


The learning process is a continuous loop of these four agents working together:


1. **Performance Agent:** Plays a game -> produces positions.
2. **Critic Agent:** Reviews the game -> assigns scores (training values) to each position.
3. **Generalizer (LMS):** Analyzes errors -> updates weights to improve the function.
4. **Experiment Generator:** Sets up the board -> starts a new game.


**Result:**
Over time, the approximation *$$\hat{V}(b)$$* becomes closer and closer to the true evaluation (**V**). The Grandmaster Agent becomes increasingly strong, just as a medical student becomes a skilled doctor after many surgeries, reviews, and corrections.

