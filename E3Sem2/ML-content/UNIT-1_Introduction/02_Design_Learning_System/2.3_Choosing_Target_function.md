# Improving Performance Reduces to Learning a Function


If we step back and connect everything from the introduction to this point, a simple but powerful idea emerges.


We began by defining learning as:
1. Performing a task **T**.
2. Improving performance with respect to a measure **P**.
3. Through accumulated experience **E**.


But how does a machine—or even a human—actually improve performance? What is the internal mechanism? The answer lies in Tom Mitchell's fundamental insight:


> **Improving performance P at a task T, in most learning problems, boils down to learning a target function.**


This is often the most misunderstood idea for beginners, so let us make it absolutely clear.


## Why Expertise Is Just a Function


When we say "learn to play chess better," "learn to drive better," or "learn to diagnose diseases better," we are not asking for magic, luck, or vague intuition. We are asking for a **consistent rule or mapping** that takes inputs from the environment and produces better decisions.


Mathematically, this rule is a function.


* **A Chess Player** improves by learning the mapping:
   **f(Board Position) -> Best Move**
* **A Doctor** improves by learning the mapping:
   **f(Symptoms) -> Diagnosis**
* **A Driver** improves by learning the mapping:
   **f(Road Conditions) -> Steering Decision**


Every form of expertise reduces to the same question: **"Given this input, what output should I choose?"**


## The Human vs. Machine Perspective


Humans store these mappings implicitly. We use experience, pattern recognition, emotional memory, and subconscious reasoning. We never feel like we are calculating a mathematical function, but cognitively, that is exactly what our brain is doing.


Machines, however, have no subconscious and no intuition. They must **explicitly** learn this function.


## The Core Insight: The ChooseMove Function


Therefore, for our specific problem of learning to play a game like checkers or chess, we can finally name the target function we are trying to learn.


We call it **ChooseMove**.


> **ChooseMove : Board -> Move**


This function takes any legal Board State (**B**) as input and outputs the Best Move (**M**).


If a learner could discover the perfect `ChooseMove` function, then:
* Every move would be optimal.
* Every position would be handled with expertise.
* The player would perform at a Grandmaster or even superhuman level.


In fact, if we had the perfect `ChooseMove` function, we would automatically have the perfect chess player. No intuition, no magic—just the correct function learned from experience.


This sets the stage for the rest of our design process. Now that we know what function we need (ChooseMove), we must figure out *how* to build it. This leads us to our next major design choice: **Choosing the Representation.**


---


# Why Scalar-Valued Functions Make Learning Easier


## The Difficulty of Discrete Learning


In the previous section, we identified `ChooseMove` as the "ideal" function because it tells us exactly what to do. However, learning `ChooseMove` directly is mathematically difficult.


Why? Because moves are **discrete**.


In chess, you either move the Knight or you don't. You cannot move "slightly more Knight." This makes the learning process "jumpy" and unstable. If the machine changes its internal parameters slightly, the output might not change at all, or it might suddenly jump to a completely different move. This lack of smooth feedback makes optimization nearly impossible.


To understand why this is a critical problem—and why **scalar (continuous) values** are the solution—consider the following analogy from medicine.


## Analogy: Treating a Disease (Discrete vs. Continuous Feedback)


Imagine a doctor treating a critically ill patient. The goal is to optimize the treatment plan to save the patient.


### Scenario 1: Only Discrete Labels
Suppose the only feedback the doctor receives about the patient’s condition is one of three labels:
1. **"Alive"**
2. **"Worse"**
3. **"Dead"**


These are discrete categories, just like discrete moves in chess.
**The Problem:** The doctor is flying blind. They cannot tell if a specific medicine is working slightly or not at all. They cannot see trends. They cannot correct small mistakes before they become fatal. By the time the feedback changes (e.g., from "Alive" to "Dead"), it is too late to learn.


### Scenario 2: Continuous Measurements (Scalar Values)
Now, imagine the doctor has access to continuous, numerical data:
* **Heart Rate:** (e.g., 72 bpm -> 85 bpm)
* **Blood Pressure:** (e.g., 120/80 -> 115/75)
* **Oxygen Saturation:** (e.g., 98% -> 92%)


These numbers change **continuously**.     
**The Result:** Now, learning and optimization are possible. The doctor can see a slight decline in blood pressure and adjust the dosage immediately. They can track smooth trends over time. Small mistakes provide early warnings, allowing for correction long before the outcome becomes fatal.


**Continuous (scalar) information creates a smooth landscape for decision-making.**


## Connecting It Back to Machine Learning


This is exactly why we use scalar-valued functions in Machine Learning.


In chess:
* **"Win / Lose / Draw"** is like **"Alive / Worse / Dead."** It is discrete, rare, and offers too little information for learning complex strategies.
* **The Evaluation Function V(b)** is like the **Medical Monitor.**


When we assign a real number to a board state: **V(b) = Real Number** (e.g., +1.5 for a slight advantage, -0.4 for a slight disadvantage), we transform the problem.


Suddenly:
1. **We can measure improvement:** Moving from a state worth +0.2 to +0.5 is a clear step forward.
2. **We can optimize:** The learning algorithm has a smooth gradient to follow, rather than abrupt jumps.
3. **We can learn from partial success:** The system learns to maximize its "score" continuously, rather than just waiting for a checkmate.


## Conclusion: The Shift to V


Because discrete outputs give almost no guidance for learning, while continuous signals give rich, usable feedback, we make a crucial design choice.


Instead of trying to learn `ChooseMove` directly, we will learn the **Evaluation Function (V)**.


Once the machine learns **V** (how to score a board), it can easily reconstruct `ChooseMove` by simply looking at the available moves and picking the one with the highest **V** score.


**Therefore, our specific Target Function for the rest of this chapter is:**
> **V : Board -> Real Number**


---


# Defining the Scoring Scheme: The "Ideal" Target Function


## The Necessity of Consistent Scoring


At this point, we fully agree: **assigning a score (a real-valued evaluation) to every board state is exactly what we need.**


This score allows the learner to:
* Compare different board states.
* Prefer better positions.
* Reject dangerous ones.
* Plan moves that lead toward high-value situations.


However, a new question immediately appears: **Which numbers should we use?**


## The Core Rule: Ordering Matters, Scales Do Not


For any "good" board state **b**, you could assign a score of +10, +0.8, +100, or +4096. Similarly, for a "bad" state, you could assign -10, -0.3, or -57.


There is no single "natural" way to choose these numbers. One evaluation function might rate a strong advantage as +5, while another rates it as +200. Both are valid as long as they follow the single most important rule of learning:


> **Good board states must receive higher scores than bad board states.**


This **ordering** is compulsory. Without this consistency, the system has no way to distinguish winning paths from losing ones, and improvement becomes impossible.


## Mitchell’s Standard Definition


To design a learning algorithm, we cannot just say "scores should be consistent." We must define the function mathematically.


For our checkers (or chess) learning system, we will use the standard definition provided by Tom Mitchell. This definition assigns values based on the **final outcome** of the game.


Let **V** be the target function. We define the value **V(b)** for any board state **b** as follows:


1. **If b is a final winning state:** V(b) = 100
2. **If b is a final losing state:** V(b) = -100
3. **If b is a final draw state:** V(b) = 0
4. **If b is a non-final state (the game is still going):**
   **V(b) = V(b')**
   *Where b' is the best final state that can be reached from b, assuming both players play optimally.*


### Understanding the Recursive Rule (V(b) = V(b'))
The first three rules are easy: if the game is over, we know the score.
The fourth rule is the tricky one. It says:


> "The value of the current board is equal to the value of the future outcome, assuming perfect play."


If you are in a position where, with perfect play, you are guaranteed to win 20 moves later, then the value of the *current* board is already +100. The value propagates backward from the future to the present.


## The Problem: Operational vs. Non-Operational


This definition of **V** is perfect. If we knew it, we would play perfectly.


However, it is **Non-Operational**.
"Non-operational" is a fancy way of saying **"We cannot actually compute it."**


To calculate **V(b)** for the current board using the definition above, the computer would have to search forward through millions of future moves to the very end of the game to find **b'**. In a complex game like chess or checkers, this is computationally impossible (due to the combinatorial explosion we discussed earlier).


## The Solution: Function Approximation


Because the **Ideal Target Function (V)** is impossible to compute directly, we must settle for the next best thing.


We task the machine learning algorithm with learning a **Target Function Approximation**, denoted as **V_hat**.


> **V_hat(b) ≈ V(b)**


Our goal is to create a function **V_hat** that:
1. **Is Operational:** It can be calculated instantly (in milliseconds) for any board state.
2. **Is Accurate:** It produces scores that are as close as possible to the true theoretical values.


This leads us to the next major design step. Since we cannot use the recursive definition (looking into the future), we must define **V_hat** by looking at the **present** properties of the board.


**This brings us to Section: Choosing the Representation of the Target Function.**



