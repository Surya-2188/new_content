# Function Approximation and the LMS Algorithm


Now that we have defined the target evaluation function (**V**) and its linear representation (**$\hat{V}$**), we must understand how learning actually happens.


In our setting, learning the function proceeds through two tightly coupled stages:


1. **Estimating training values** for board states (especially intermediate ones).
2. **Updating the weights (w)** of the evaluation function so that the approximation (**$\hat{V}$**) becomes accurate.


These two processes repeat over many games, gradually improving the system’s understanding of how good or bad each board position is.


---


## Stage 1: Assigning Training Values


### Terminal States (Easy, Almost No Learning Needed)
Assigning a training value to a terminal board state is trivial:


* **Win:** (+100)
* **Loss:** (-100)
* **Draw:** (0)


Even someone who does not know chess can look at the final board and say: *"This is a win. This is a loss."*


These values require no learning, yet they are extremely important because **Terminal states act as ground-truth anchors**. They are the only places in the game where we know the evaluation perfectly.


### Intermediate States (Hard, No Direct Labels)
For states during the game, we do NOT know if they are truly good or bad.
* A move that seems fine early may lead to a loss later.
* A seemingly bad move may still allow recovery.



These states require estimated training values. The idea is to use the **Successor Rule**:


**$V_{\text{train}}(b) = \hat{V}(\text{Successor}(b))$**



This means we use the **current approximation ($\hat{V}$)** to label the earlier board state. This is called **Bootstrapping**: The learner uses its own current knowledge to improve its future knowledge.


### Why Bootstrapping Makes Sense
1. **Terminal states have perfect labels:** So **$\hat{V}$** rapidly becomes very accurate there.
2. **States close to terminal positions have simple structure:** It becomes easy for the evaluation function to get them right early.
3. **Accuracy propagates backward:** Through the successor-value rule, the accuracy flows from the end of the game back to the beginning.


This clarifies why **$\hat{V}$** tends to be more accurate for board states closer to the game’s end. This improvement is not due to the math of the update rule itself, but because terminal states provide ground truth, and near-terminal states inherit that truth.


---


## Stage 2: Updating the Weights (Learning $\hat{V}$)


Once training values (**V_train**) are assigned, we adjust the weights so that the evaluation function matches these values as closely as possible.


We use the **Least Mean Squares (LMS) Update Rule**:

$$
w_i \leftarrow w_i + \eta \left( V_{\text{train}}(b) - \hat{V}(b) \right) x_i
$$



Let us now understand this rule in depth.


### Understanding the LMS Weight Update Rule


#### 1. When the Error Is Zero: No Change
If **$V_{\text{train}}$
 = $\hat{V}$**, then the update is zero.
* **Result:** No correction is needed.
* **Analogy:** This is like a student who solved a question perfectly and receives no red marks.


#### 2. When the Prediction Is Too Low
(The board is actually better than $\hat{V}$ thinks).
If **($V_{\text{train}}$ - $\hat{V}$) > 0**, the error is positive.
* **Result:** We increase the weight.
* **Interpretation:** The system undervalued this position. It must increase the contribution of the present features.
* **Teacher Analogy:** "Your answer is too low—pay more attention to the important concepts."


#### 3. When the Prediction Is Too High
(The board is actually worse than $\hat{V}$ thinks).
If **($V_{\text{train}}$- $\hat{V}$) < 0**, the error is negative.
* **Result:** We decrease the weight.
* **Interpretation:** The system overestimated this state. It must decrease the influence of these features.
* **Teacher Analogy:** "You placed too much importance on these ideas—reduce their weight."


#### 4. Features Not Present Do NOT Receive Updates
If the feature value **x = 0**, then the weight does not change.
* **Result:** Only features that actually appear in the board influence weight updates.
* **Analogy:** If you never used a concept in a problem, the teacher won’t correct that concept.


---


## Why LMS Actually Works


Despite its simplicity, LMS is mathematically powerful. By repeatedly adjusting weights in the direction that reduces error, the algorithm converges toward the **least-squares optimal approximation** of the true target values.


This mirrors how students learn:
1. Fix one mistake at a time.
2. Gradually reduce overall error.
3. Eventually achieve correct reasoning across all examples.


### LMS as Gradient Descent
The goal is to minimize the **Squared Error (E)**:


> **E = Sum of ($V_{\text{train}}$- $\hat{V}$)^2**


The LMS rule is equivalent to performing incremental **Gradient Descent** on this error surface. It is:
* **Online:** Updates as examples arrive.
* **Efficient:** Computationally cheap.
* **Robust:** Can handle noisy labels.


---


## Final Intuition: How LMS Learns Like Humans


Learning happens through:
1. **Raising weights** when the system underestimates.
2. **Lowering weights** when it overestimates.
3. **Ignoring** irrelevant features.
4. **Gradually refining** evaluations over time.


This mirrors exactly how humans improve judgment in chess, medicine, sports, and academics.


### Chapter Summary
Function approximation in our game-learning system involves:
* **Estimating training values:** Exact values for terminal states, bootstrapped estimates for intermediate states.
* **Adjusting weights using LMS:** Correcting prediction errors to move toward optimal values.


This two-stage process enables the system to learn a powerful evaluation function capable of guiding move selection—exactly like how human expertise develops.

